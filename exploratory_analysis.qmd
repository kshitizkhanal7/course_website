---
title: "Exploratory Data Analysis"
editor: visual
---

We frequently work on datasets created by others for their own needs. We gather some of those datasets when accessible to us, and explore whether the data can be useful to us or whether we can dig up useful insights from data. This process if often called Exploratory Data Analysis, and is one of the most important and frequently performed activities in (urban) analytics.

Borrowing from R for Data Science (1e), 

EDA is an iterative cycle in which we:

* Generate questions about data.
* Search for answers by visualizing, transforming, and modelling data.
* Use what we learn to refine your questions and/or generate new questions.

There is no exact recipe for EDA, as it depends on answers we seek and the nature of the data. However, some operations are more frequent than others in typical EDA. 

* Understanding data
* Checking data quality
* Cleaning data to fit our needs
* Transforming, visualizing, and modeling data

Essentially, you perform various analytical operations to ask questions to your data depending on your need and the nature of the data. EDA is a Socratic approach in that sense.

## Introduction to data

SFpark is the parking demand management program in San Francisco. The data we're using are entries and exits from public parking garages from 2011-2013, available from https://www.sfmta.com/getting-around/drive-park/demand-responsive-pricing/sfpark-evaluation

First, we need to load the libraries we will be using.

```{r, message=FALSE}
library(tidyverse)  #library for dealing with tabular data
library(lubridate)  #library for handling dates
```

The data are packaged as a CSV (comma separated values) file, which is just a text file where each line is a row, and the columns are separated by commas.

```{r}
df <- read_csv("https://github.com/kshitizkhanal7/plan372-exercises/raw/main/sfpark/data/sfpark.csv")
```

### A peek at the data
```{r, collapse=TRUE}
head(df)
```
We have six columns:

 date: the date the data were recorded

 facility: the garage where the data were recorded

 district: the district (neighborhood) where the garage is

 usage_type: the type of payment (e.g. monthly pass, transient/hourly)

 entries, exits: the number of cars that entered and exited the garage that day that used that type of payment
 
Exercise: Another function to understand a data better glimpse(). Use the function and compare it with head(). What do you find? 
 
## Tests for data quality

We need to ensure the data is fit-for-purpose. We can check for completeness, duplication, and consistency.

```{r, collapse=TRUE}
# Checking missing values in each column
colSums(is.na(df))
```
This is a very good dataset in terms of completeness. There are no NA values. 

```{r, collapse=TRUE}
# Checking for duplicate records
duplicated_records <- df %>%
  filter(duplicated(.))

duplicated_records
```

There are zero duplicate rows. All observations are unique.

We can also check if observations have values we can expect.

```{r, collapse=TRUE}
# Checking consistency of Age column
df %>%
  filter(entries < 0 | is.integer(entries))
```

Number of entries is not negative nor are the entries decimals.

## Start with the simple questions about data

#### What garages do we have?

```{r, collapse=TRUE}
unique(df$facility)
```

Exercise: 

 * Where in San Francisco are they (in which districts)?

 * In your own words, what does a row of the data represent?

#### How many cars enter a garage on an average day

```{r, collapse=TRUE}
mean(df$entries)
```

Exercise: That may be skewed by outliers. What is the median?

## Understanding group characteristics

Grouped data analysis is a very common pattern - rather than a mean over the entire dataset, we may want a mean by groups. For instance, the median being so different from the mean suggests outliers - perhaps one very large garage. Let's look at the mean entries by garage.

```{r, collapse=TRUE}
df |> 
  group_by(facility) |> 
  summarize(mean_entries=mean(entries))
```

But this could be misleading. Does each row represent a single day at a single garage?

Are we actually computing the mean of above?

We can use the group_by and summarize functions to create a dataset that shows total entries and exits for each day, and create a new table with this information.

```{r, collapse=TRUE}
total_entries <- df |>
  group_by(date, facility) |> 
  summarize(entries=sum(entries), exits=sum(exits))

head(total_entries)
```

Now we can calculate the mean entries by garage, using this new dataset.

## Focusing on what's interesting to us

Maybe we don't want to look at the entire city, but only at garages in the Civic Center neighborhood. We can filter the data and repeat the above analysis.

```{r, collapse=TRUE}
civic_center_data <- filter(df, district == "Civic Center")
```


Now, repeat the above analysis to compute total entries by day, and take the daily average, using only data from Civic Center use group_by and summarize to compute means.

Repeat the process, looking only at garages in the Mission

## Handling dates

Another dimension of this dataset that we have not explored is the date column. Before we can work with dates, however, we have to parse the column.

Every column in a table has a type (for instance, integer, decimal number, string). read_csv tries to guess the best type for each field. We can see what types are used in our table by printing the table to the console. The types are printed just below the column names.

```{r, collapse=TRUE}
head(total_entries)
```

read_csv has read the date as string. These columns only store the letters, and don't know that they represent dates. Before we can use the dates, we need to parse the dates. To parse dates, we will use the lubridate library: https://lubridate.tidyverse.org/

* the mdy function parses dates in month/day/year format

```{r, collapse=TRUE}
total_entries <- mutate(total_entries, date=mdy(date))
```

Display the table again, to check the types

```{r, collapse=TRUE}
head(total_entries)
```

Let's look at the mean entries by year, to look for trends over time

First, we need to extract the year from the date column, and assign it to a new column. We previously used mutate for this; this line uses $ notation which is another way of doing the same thing.

```{r, collapse=TRUE}
total_entries$year = year(total_entries$date)
```

Make sure it worked
```{r, collapse=TRUE}
unique(total_entries$year)
```

Now, look at mean entries by garage and year

```{r, collapse=TRUE}
group_by(total_entries, facility, year) |>
  summarize(entries=mean(entries))
```

The year_totals table is in "long" format - one row for each facility for each year. It would be easier to read in "wide" format: one row for each facility and one column for each year.

The column names are taken from the field named in names_from, and the values from the field named in values_from now, look at mean entries by garage and year.

```{r, collapse=TRUE}
group_by(total_entries, facility, year) |>
  summarize(entries=mean(entries)) |>
  pivot_wider(names_from=year, values_from=entries)
```

Exercise: repeat the above, but get the per-month mean rather than per year, to show seasonal trends. You should have a table with rows for each garage and columns for January, February, etc.

## Testing our hunches
Weekdays vs weekends 
Some garages may be more popular on weekdays than on weekends. We need to repeat the above analysis, but we want a mean for weekends and a mean for weekdays. Lubridate does not have a  weekend function, so we need to use the day of week function to make a new weekend column. First, we can create a day of week column.

```{r, collapse=TRUE}
total_entries$day_of_week <- wday(total_entries$date, label=T)
```

Look at how the days of week are abbreviated

```{r, collapse=TRUE}
unique(total_entries$day_of_week)
```

now, we can recode that to weekday/weekend

```{r, collapse=TRUE}
total_entries$weekend <- recode(total_entries$day_of_week, 
                               "Mon"="weekday", 
                               "Tue"="weekday", 
                               "Wed"="weekday", 
                               "Thu"="weekday",
                               "Fri"="weekday", 
                               "Sat"="weekend", 
                               "Sun"="weekend")

head(total_entries$weekend)
```

Calculate the means

```{r, collapse=TRUE}
group_by(total_entries, facility, weekend) |>
  summarize(entries=mean(entries)) |> 
  pivot_wider(names_from=weekend, values_from=entries)
```

Exercise: Compute means by season

## Visualization

This data would be more interesting if it were on a graph. To make graphs in R, most people use the ggplot2 library, which is part of tidyverse.

Let us create a dataset that shows how many entries to all garages there were on each day

```{r, collapse=TRUE}
citywide_entries = group_by(total_entries, date) |> 
  summarize(entries=sum(entries))
```

Create a plot using the total_entries table. We define an "aesthetic" that the date will be the x axis, and the number of entries will be the y axis

```{r, collapse=TRUE}
ggplot(citywide_entries, aes(x=date, y=entries)) +
  geom_line()
```

Wow, that's a mess - there's too much day-to-day variation to understand trends. Let's group the data by month and year, and plot again

*  the floor_date function returns the beginning of whatever period you put in - in this case, the beginning of the month

```{r, collapse=TRUE}
citywide_entries$month_year <- floor_date(citywide_entries$date, unit="month")

monthly_entries = group_by(citywide_entries, month_year) |> summarize(entries=sum(entries))

ggplot(monthly_entries, aes(x=month_year, y=entries)) + 
  geom_line()
```

What month has the highest parking demand? Why do you think that is?

This is an interesting plot, but it would be more interesting to see it by garage. We can do this by using data that is not summed up to citywide level, an then telling ggplot to group by the facility

```{r, collapse=TRUE}
total_entries$year_month = floor_date(total_entries$date, unit="month")

garage_month_entries = group_by(total_entries, facility, year_month) |> 
  summarize(entries=sum(entries))
```

look at the result of that call

```{r, collapse=TRUE}
garage_month_entries[1:20,] #looking at the first 20 observations
```

Now, plot the data but tell ggplot to group by facility, use separate colors for each facility

```{r, collapse=TRUE}
ggplot(garage_month_entries, aes(x=year_month, y=entries, group=facility, color=facility)) +
  geom_line()
```

Which garages have more of a seasonal trend? why?

Exercise: look at weekly trends rather than monthly trends

#### A closer look at Usage types 

The data include several usage types - monthly passes, transient users, etc.

Exercise: first, extract all of the unique values of the usage_type column to see what the possibilities are.

* How many entries are from each of the usage types? What usage type is most common?

* Are the patterns of usage types different on the weekdays vs. the weekends?

* Make a plot of the monthly trend in number of entries by usage type.


Credits: This tutorials borrows heavily and expands on EDA tutorial taught in PLAN 372 Spring 2023 by Matthew Wigginton Bhagat-Conway.